{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the Data using AI\n",
    "\n",
    "**Description:**\n",
    "\n",
    "In this section, we utilize AI to clean and preprocess the job market data, implementing optimizations to reduce API usage and ensure data quality. The process involves simplifying roles, skills, and languages using a language model with a caching mechanism to improve efficiency.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Setup and Configuration:**\n",
    "   - Initialize the language model and caching system\n",
    "   - Define standardized categories for skills and programming languages\n",
    "   - Implement error handling for various data types and edge cases\n",
    "\n",
    "2. **Caching Mechanism:**\n",
    "   - Use a file-based cache to store previously processed items\n",
    "   - Avoid redundant API calls for repeated terms\n",
    "   - Persist cache between runs for continuous optimization\n",
    "\n",
    "3. **Test with Random Sample:**\n",
    "   - Select a random subset of 5 entries for initial testing\n",
    "   - Validate the cleaning process on diverse data points\n",
    "   - Review results before processing the full dataset\n",
    "\n",
    "4. **Process the Data:**\n",
    "   - Clean skills and languages using the optimized classifier\n",
    "   - Handle missing values and invalid entries gracefully\n",
    "   - Process data in batches with appropriate rate limiting\n",
    "\n",
    "5. **Data Transformation:**\n",
    "   - Split combined skill entries into distinct categories\n",
    "   - Standardize terminology for consistency\n",
    "   - Remove duplicates within each category\n",
    "\n",
    "6. **Quality Assurance:**\n",
    "   - Maintain original data while adding cleaned columns\n",
    "   - Track and report processing status\n",
    "   - Handle edge cases and errors without interrupting the process\n",
    "\n",
    "7. **Save and Export:**\n",
    "   - Add cleaned data as new columns in the DataFrame\n",
    "   - Export the enhanced dataset to a new CSV file\n",
    "   - Preserve both original and cleaned data for reference\n",
    "\n",
    "This approach combines AI-powered cleaning with practical optimizations, ensuring efficient processing while maintaining data quality. The implementation includes safeguards against API rate limits and mechanisms to handle various data inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import time\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0, model=\"gpt-4-0125-preview\", \n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Define lists for classification\n",
    "roles = ['data scientist', 'data engineer', 'analyst', 'mle', 'manager', 'director', 'na']\n",
    "skills = [\n",
    "    'statistics', 'machine_learning', 'data_analysis', 'data_mining',\n",
    "    'nlp', 'computer_vision', 'deep_learning', 'big_data', 'na'\n",
    "]\n",
    "languages = [\n",
    "    'python', 'r', 'matlab', 'java', 'c++', 'sas', 'na'\n",
    "]\n",
    "\n",
    "class SimpleCache:\n",
    "    def __init__(self, cache_file='classification_cache.json'):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = self.load_cache()\n",
    "        \n",
    "    def load_cache(self):\n",
    "        try:\n",
    "            with open(self.cache_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            return {'roles': {}, 'skills_languages': {}}\n",
    "    \n",
    "    def save_cache(self):\n",
    "        with open(self.cache_file, 'w') as f:\n",
    "            json.dump(self.cache, f)\n",
    "    \n",
    "    def get_role(self, text):\n",
    "        # Handle NaN, None, and non-string types\n",
    "        if pd.isna(text) or text is None:\n",
    "            return 'na'\n",
    "        text_str = str(text).lower().strip()\n",
    "        return self.cache['roles'].get(text_str)\n",
    "    \n",
    "    def get_skill_language(self, text):\n",
    "        # Handle NaN, None, and non-string types\n",
    "        if pd.isna(text) or text is None:\n",
    "            return ('unknown', 'na')\n",
    "        text_str = str(text).lower().strip()\n",
    "        return self.cache['skills_languages'].get(text_str)\n",
    "    \n",
    "    def set_role(self, text, value):\n",
    "        # Handle NaN, None, and non-string types\n",
    "        if pd.isna(text) or text is None:\n",
    "            return\n",
    "        text_str = str(text).lower().strip()\n",
    "        self.cache['roles'][text_str] = value\n",
    "        self.save_cache()\n",
    "    \n",
    "    def set_skill_language(self, text, value):\n",
    "        # Handle NaN, None, and non-string types\n",
    "        if pd.isna(text) or text is None:\n",
    "            return\n",
    "        text_str = str(text).lower().strip()\n",
    "        self.cache['skills_languages'][text_str] = value\n",
    "        self.save_cache()\n",
    "\n",
    "# Initialize cache\n",
    "cache = SimpleCache()\n",
    "\n",
    "def simplify_role(text):\n",
    "    \"\"\"Simplify role text using cache first\"\"\"\n",
    "    # Handle NaN values\n",
    "    if pd.isna(text) or text is None:\n",
    "        return 'na'\n",
    "    \n",
    "    # Check cache first\n",
    "    cached_result = cache.get_role(text)\n",
    "    if cached_result:\n",
    "        return cached_result\n",
    "    \n",
    "    # If not in cache, use API\n",
    "    try:\n",
    "        schema = {\n",
    "            \"title\": \"role_selector\",\n",
    "            \"description\": (\n",
    "                \"Match job roles to standard titles, being tolerant of typos and variations:\\n\"\n",
    "                \"Examples:\\n\"\n",
    "                \"'senior data scientist' → 'data scientist'\\n\"\n",
    "                \"'ml engineer' → 'mle'\\n\"\n",
    "                \"'data analytics manager' → 'manager'\\n\"\n",
    "                \"'chief data officer' → 'director'\\n\"\n",
    "                f\"Available roles: {roles}\\n\"\n",
    "                \"Return 'na' only if the role is completely unclear\"\n",
    "            ),\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"selected_role\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": roles,\n",
    "                    \"description\": \"Most similar role from the list, or 'na' if unclear\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"selected_role\"]\n",
    "        }\n",
    "        llm_struc = llm.with_structured_output(schema)\n",
    "        result = llm_struc.invoke(str(text))\n",
    "        simplified = result[\"selected_role\"]\n",
    "        \n",
    "        # Cache the result\n",
    "        cache.set_role(text, simplified)\n",
    "        return simplified\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing role {text}: {str(e)}\")\n",
    "        return 'na'\n",
    "\n",
    "def simplify_text_v2(text):\n",
    "    \"\"\"Enhanced version with caching\"\"\"\n",
    "    # Handle NaN values\n",
    "    if pd.isna(text) or text is None:\n",
    "        return (\"unknown\", \"na\")\n",
    "    \n",
    "    # Check cache first\n",
    "    cached_result = cache.get_skill_language(text)\n",
    "    if cached_result:\n",
    "        return tuple(cached_result) if isinstance(cached_result, list) else cached_result\n",
    "    \n",
    "    # If not in cache, use API\n",
    "    try:\n",
    "        schema = {\n",
    "            \"title\": \"skill_language_classifier\",\n",
    "            \"description\": (\n",
    "                \"Classify and standardize the input text:\\n\"\n",
    "                \"1. First determine if it's a skill or programming language\\n\"\n",
    "                \"2. Then match to the standard list\\n\"\n",
    "                f\"Skills: {skills}\\n\"\n",
    "                f\"Languages: {languages}\\n\"\n",
    "                \"Examples:\\n\"\n",
    "                \"'statistical analysis' → ('skill', 'statistics')\\n\"\n",
    "                \"'python coding' → ('language', 'python')\\n\"\n",
    "                \"'machine learning exp' → ('skill', 'machine_learning')\"\n",
    "            ),\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"category\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"skill\", \"language\", \"unknown\"],\n",
    "                    \"description\": \"The category that best matches the input\"\n",
    "                },\n",
    "                \"standardized\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": skills + languages,\n",
    "                    \"description\": \"The standardized term from the appropriate list\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"category\", \"standardized\"]\n",
    "        }\n",
    "        llm_struc = llm.with_structured_output(schema)\n",
    "        result = llm_struc.invoke(str(text))\n",
    "        output = (result[\"category\"], result[\"standardized\"])\n",
    "        \n",
    "        # Cache the result\n",
    "        cache.set_skill_language(text, list(output))\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {text}: {str(e)}\")\n",
    "        return (\"unknown\", \"na\")\n",
    "\n",
    "def process_data(df, batch_size=50, rate_limit_delay=1):\n",
    "    clean_roles = []\n",
    "    clean_skills = []\n",
    "    clean_languages = []\n",
    "    \n",
    "    # Process roles\n",
    "    print(\"\\nProcessing Roles:\")\n",
    "    for i, role_text in enumerate(df['roles']):\n",
    "        if i > 0 and i % batch_size == 0:\n",
    "            print(f\"Processed {i} roles...\")\n",
    "            time.sleep(rate_limit_delay)\n",
    "        \n",
    "        simplified_role = simplify_role(role_text)\n",
    "        clean_roles.append(simplified_role)\n",
    "    \n",
    "    # Process skills\n",
    "    print(\"\\nProcessing Skills and Languages:\")\n",
    "    for i, skills_text in enumerate(df['skills']):\n",
    "        if i > 0 and i % batch_size == 0:\n",
    "            print(f\"Processed {i} skill sets...\")\n",
    "            time.sleep(rate_limit_delay)\n",
    "        \n",
    "        if pd.isna(skills_text):\n",
    "            clean_skills.append('')\n",
    "            clean_languages.append('')\n",
    "            continue\n",
    "            \n",
    "        skill_items = str(skills_text).split(', ')\n",
    "        skills_list = []\n",
    "        languages_list = []\n",
    "        \n",
    "        for skill in skill_items:\n",
    "            category, simplified = simplify_text_v2(skill)\n",
    "            if simplified != 'na':\n",
    "                if category == 'skill':\n",
    "                    skills_list.append(simplified)\n",
    "                elif category == 'language':\n",
    "                    languages_list.append(simplified)\n",
    "        \n",
    "        clean_skills.append(', '.join(set(skills_list)))\n",
    "        clean_languages.append(', '.join(set(languages_list)))\n",
    "    \n",
    "    return clean_roles, clean_skills, clean_languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with a random subset...\n",
      "\n",
      "Processing Roles:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yihuang/projects/data_science/Week_3/myenv/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:1645: UserWarning: Cannot use method='json_schema' with model gpt-4-0125-preview since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Skills and Languages:\n",
      "\n",
      "Test Results:\n",
      "                             roles      clean_role  \\\n",
      "628  Data Scientist with GCP Cloud  data scientist   \n",
      "631                 Data Scientist  data scientist   \n",
      "741                 Data Scientist  data scientist   \n",
      "514  Data Scientist (Supply Chain)  data scientist   \n",
      "365                 Data Scientist  data scientist   \n",
      "\n",
      "                                                skills  \\\n",
      "628  Data Science, GCP, Natural Language Processing...   \n",
      "631                                     Research, Data   \n",
      "741  Statistical modeling, tableau, Coding, Analyti...   \n",
      "514  Supply chain management, data science, Neural ...   \n",
      "365  Computer science, data science, Data modeling,...   \n",
      "\n",
      "                                          clean_skills clean_languages  \n",
      "628  nlp, computer_vision, machine_learning, deep_l...                  \n",
      "631                                                                     \n",
      "741        machine_learning, data_analysis, statistics                  \n",
      "514     machine_learning, data_analysis, deep_learning          matlab  \n",
      "365          machine_learning, data_analysis, big_data                  \n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('DataScience_jobs.csv')\n",
    "\n",
    "# Test with random subset\n",
    "print(\"Testing with a random subset...\")\n",
    "df_test = df.sample(n=5, random_state=42)  # Random 5 rows, set random_state for reproducibility\n",
    "clean_roles, clean_skills, clean_languages = process_data(df_test)\n",
    "\n",
    "# Show test results\n",
    "df_test['clean_role'] = clean_roles\n",
    "df_test['clean_skills'] = clean_skills\n",
    "df_test['clean_languages'] = clean_languages\n",
    "print(\"\\nTest Results:\")\n",
    "print(df_test[['roles', 'clean_role', 'skills', 'clean_skills', 'clean_languages']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the full dataset\n",
    "print(\"Processing full dataset...\")\n",
    "clean_roles, clean_skills, clean_languages = process_data(df)\n",
    "\n",
    "# Add cleaned data to DataFrame\n",
    "df['clean_role'] = clean_roles\n",
    "df['clean_skills'] = clean_skills\n",
    "df['clean_languages'] = clean_languages\n",
    "\n",
    "# Save the cleaned data\n",
    "output_file = 'DataScience_jobs_cleaned_all.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\nFull dataset processed and saved to {output_file}\")\n",
    "\n",
    "# Show a sample of the final results\n",
    "print(\"\\nSample of final cleaned data:\")\n",
    "print(df[['roles', 'clean_role', 'skills', 'clean_skills', 'clean_languages']].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
